---
title: "Exploring Bike Share Availability"
output: html_notebook
---

## Problem to Solve

It is really annoying to reach a bikeshare station and have no bikes available! We want to be able to predict how many bikes will be at a given station, and allow those who care about it (ME) to explore those predictions and then regularly take actions on them.

We start with data:

```{r get-data}
library(tidyverse)
library(odbc)
library(dbplyr)
con <- dbConnect(odbc::odbc(), "Content DB", timeout = 10)
bikes <- dplyr::tbl(con, "bike_raw_data")
stations <- dplyr::tbl(con, "bike_station_info")
all_data <- bikes %>% 
    group_by(
        id = station_id, 
        hour = hour(time), 
        date = date(time), 
        month = month(time), 
        dow = TRIM(to_char(time, "Day"))
    ) %>%
    summarize(
        n_bikes = mean(num_bikes_available, na.rm = TRUE)
    ) %>%
    inner_join(
        select(stations, id = station_id, lat, lon)
    )
all_data
```


Let's take a look at what we have, noticing two characteristics:

- Spatial data  
- Temporal data 

```{r first-look}
# lets look at the average number by date, simplifying our hourly data at first
all_data %>% 
    filter(id == "1") %>%
    group_by(date,id) %>% 
    summarize(avg = mean(n_bikes)) %>% 
    ggplot(aes(date, avg)) + 
    geom_point() + 
    geom_line(alpha = 0.5) + 
    theme_minimal() + 
    labs(
        title = "Bikes available overtime at station 1"
    )
```

```{r}
library(leaflet)

all_stations <- stations %>% 
  collect() 


all_stations %>%
    leaflet() %>%
    addProviderTiles(providers$CartoDB.Positron) %>%
    setView(lng = median(all_stations$lon), lat = median(all_stations$lat), zoom = 14) %>%
    addAwesomeMarkers(
        lng = ~lon,
        lat = ~lat,
        icon = awesomeIcons(
            "bicycle",
            library = "fa",
            iconColor = "white",
            markerColor = "red"
        ),
        label = ~paste0(name)
    )
```

## Model Exploration

Now lets naively see if we can fit a model against our data.

There are many things we could do with spatio-temporal data modelling. For now, we'll start with pretty naive approach, which is to use a training/testing split on time, and we'll include location as a predictor. This approach doesn't utilize a lot of the structure in our data, so we'll start with a pretty "black box" model, in this case gradient boosted trees. 

Keep in mind, our goal is to share early and often, to be sure we are tackling the right problem, before we try to optimize our model.

```{r}
split_date <- lubridate::ymd("2020-04-01")
station_sample <- as.character(1:10)
train <- all_data %>% 
    filter(id %in% station_sample) %>% 
    filter(date < split_date) %>% 
    collect()

test <- all_data %>% 
    filter(id %in% station_sample) %>% 
    filter(date > split_date) %>% 
    collect()
    
library(tidymodels)
library(parsnip)
library(recipes)
library(workflows)
data_prep <- recipe(n_bikes ~ id + dow + hour, data = train) %>%
  step_dummy(dow) %>%
  step_dummy(id) %>% 
  prep(train)


gbt <- boost_tree(mode = "regression") %>% 
    set_engine("xgboost")

gbt_pipeline <- workflow() %>% 
    add_recipe(data_prep) %>% 
    add_model(gbt)

model_fit <- gbt_pipeline %>% 
    fit(data = train)

results <- predict(model_fit, test) %>% 
    bind_cols(test)

# not a great model
ggplot(results, aes(.pred, n_bikes)) + 
    geom_point()

library(yardstick)
results %>% 
    rsq(truth = n_bikes, estimate = .pred)
```

```{r save-model}
saveRDS(object = model_fit, "model.RDS")
```

What other things will we need to figure out?

- How to update our data regularly
- How to update our model? Is it still good?
- How to expose our predictions

But we don't need to eat the elephant at once. Let's start by sharing this exploratory work to be sure we are on the right track!
